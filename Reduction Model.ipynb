{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPujoVD86fev9DxtJhRdu3K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PKregRVGCJim"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","source":["%cd /gdrive/My Drive/Tesi Notebooks"],"metadata":{"id":"4rve5DBwCl1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Data quality libraries\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import random\n","import json\n","import datetime\n","import time\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","plt.rc('font', size=16)\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.linear_model import LinearRegression\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.multioutput import RegressorChain\n","from sklearn.svm import LinearSVR\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedKFold\n","from numpy import absolute\n","from numpy import mean\n","from numpy import std\n","import warnings\n","import logging\n","os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","print(tf.__version__)"],"metadata":{"id":"41sGYn-6CpMw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Random seed for reproducibility\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)"],"metadata":{"id":"mTALubX5Cq7O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_ResNet(input_shape, classes):\n","    n_feature_maps = 64\n","    input_layer = tfkl.Input(input_shape)\n","\n","    # BLOCK 1\n","\n","    conv_x = tfkl.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n","    conv_x = tfkl.BatchNormalization()(conv_x)\n","    conv_x = tfkl.Activation('relu')(conv_x)\n","\n","    conv_y = tfkl.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n","    conv_y = tfkl.BatchNormalization()(conv_y)\n","    conv_y = tfkl.Activation('relu')(conv_y)\n","\n","    conv_z = tfkl.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n","    conv_z = tfkl.BatchNormalization()(conv_z)\n","\n","    # expand channels for the sum\n","    shortcut_y = tfkl.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n","    shortcut_y = tfkl.BatchNormalization()(shortcut_y)\n","\n","    output_block_1 = tfkl.add([shortcut_y, conv_z])\n","    output_block_1 = tfkl.Activation('relu')(output_block_1)\n","\n","    # BLOCK 2\n","\n","    conv_x = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n","    conv_x = tfkl.BatchNormalization()(conv_x)\n","    conv_x = tfkl.Activation('relu')(conv_x)\n","\n","    conv_y = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n","    conv_y = tfkl.BatchNormalization()(conv_y)\n","    conv_y = tfkl.Activation('relu')(conv_y)\n","\n","    conv_z = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n","    conv_z = tfkl.BatchNormalization()(conv_z)\n","\n","    # expand channels for the sum\n","    shortcut_y = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n","    shortcut_y = tfkl.BatchNormalization()(shortcut_y)\n","\n","    output_block_2 = tfkl.add([shortcut_y, conv_z])\n","    output_block_2 = tfkl.Activation('relu')(output_block_2)\n","\n","    # BLOCK 3\n","\n","    conv_x = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n","    conv_x = tfkl.BatchNormalization()(conv_x)\n","    conv_x = tfkl.Activation('relu')(conv_x)\n","\n","    conv_y = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n","    conv_y = tfkl.BatchNormalization()(conv_y)\n","    conv_y = tfkl.Activation('relu')(conv_y)\n","\n","    conv_z = tfkl.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n","    conv_z = tfkl.BatchNormalization()(conv_z)\n","\n","    # no need to expand channels because they are equal\n","    shortcut_y = tfkl.BatchNormalization()(output_block_2)\n","\n","    output_block_3 = tfkl.add([shortcut_y, conv_z])\n","    output_block_3 = tfkl.Activation('relu')(output_block_3)\n","\n","    # FINAL\n","    gap_layer = tfkl.GlobalAveragePooling1D()(output_block_3)\n","\n","    output_layer = tfkl.Dense(classes, activation='softmax')(gap_layer)\n","\n","    model = tfk.models.Model(inputs=input_layer, outputs=output_layer)\n","\n","    # Compile the model\n","    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n","\n","    # Return the model\n","    return model"],"metadata":{"id":"9uK_ReibCzJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_values(x_values_v, y_values_v):\n","  model = LinearRegression()\n","  x_values = np.array(x_values_v).reshape(-1, 1)\n","  model.fit(np.log(x_values), y_values_v)\n","  C1 = model.intercept_\n","  C2 = model.coef_[0]\n","\n","  x_list = [round(0.1 * i, 1) for i in range(2, 11)] #To produce a list of values from 0.2 to 1.0\n","  values = C1 + C2 * np.log(x_list)\n","\n","  return values"],"metadata":{"id":"ZRCEm8FLC6Iq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_values(file_path):\n","  with open(file_path, \"r\") as json_file:\n","    data = json.load(json_file)\n","\n","  x_values = []\n","  y_values = []\n","  if 'horizontal' in file_path:\n","    for object_experiment in data:\n","      y_values.append(object_experiment[\"Accuracy_6\"])\n","      x_values.append(object_experiment[\"data_volume\"])\n","  elif 'vertical' in file_path:\n","    for object_experiment in data:\n","      y_values.append(object_experiment[\"Accuracy_6\"])\n","      x_values.append((0.1*object_experiment[\"num_clients\"]))\n","\n","  y_val = create_values(x_values, y_values) #It retrieves list of values from the curve\n","  return y_val"],"metadata":{"id":"Sg7lV1qsC8LY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_dataframe(file_path, meta_data, dataframe, extend):\n","  with open(file_path, \"r\") as json_file:\n","    data = json.load(json_file)\n","\n","  for object_exp in data:\n","    first_meta = meta_data[:]\n","    items_to_append = [object_exp['Accuracy_6'], object_exp['num_clients'], object_exp['data_volume']]\n","    first_meta.extend(items_to_append)\n","    dataframe.loc[len(dataframe)] = first_meta\n","\n","  if extend:\n","    values = get_values(file_path)\n","    volume = [i for i in range(2, 11)] # 2, 3, 4,...10\n","    print(volume)\n","    for object_val, q_volume in zip(values, volume):\n","      if object_val > 1.0:\n","        object_val = 0.99\n","      first_meta = meta_data[:]\n","      if 'horizontal' in file_path:\n","        items_to_append = [object_val, 10, round(0.1  *q_volume, 1)]\n","      if 'vertical' in file_path:\n","        items_to_append = [object_val, q_volume, 1.0]\n","      first_meta.extend(items_to_append)\n","      dataframe.loc[len(dataframe)] = first_meta\n","\n","  return dataframe"],"metadata":{"id":"G4JxVswpC85s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train a Random Forest Algorithm to define a model able to define the number of clients and the data volume per each client given a treashold accuracy value\n","\n","\n","#Create a dataframe with named columns\n","columns = ['dataset_type', 'dataset_length', 'length_sequence', 't_model', 'num_param', 'tot_clients', 'accuracy', 'num_clients', 'data_volume']\n","dataframe = pd.DataFrame(columns=columns)\n","\n","#Info about ChlorineConcentration\n","data_chlo = ['Sensor', 3740, 166, 'Resnet', model_info.count_params(), 10] #model_info.count_params should be fixed\n","data_star = ['Sensor', 7236, 1024, 'Resnet', model_info.count_params(), 10]\n","\n","files = ['/gdrive/My Drive/Tesi Notebooks/ChlorineConcentration/results/Data Volume/horizontal_results.json', '/gdrive/My Drive/Tesi Notebooks/ChlorineConcentration/results/Data Volume/vertical_results.json', '/gdrive/My Drive/Tesi Notebooks/StarLightCurves/results/Data Volume/results_horizontal.json', '/gdrive/My Drive/Tesi Notebooks/StarLightCurves/results/Data Volume/results_vertical.json']\n","\n","for file_path in files:\n","  if 'ChlorineConcentration' in file_path:\n","    dataframe = create_dataframe(file_path, data_chlo, dataframe, True)\n","  elif 'StarLightCurves' in file_path:\n","    dataframe = create_dataframe(file_path, data_star, dataframe, True)\n","\n","\n","target_columns = columns[-2:]\n","input_feature = columns[:-2]\n","X_train_dataframe = dataframe.drop(columns=target_columns)\n","Y_train_dataframe = dataframe.drop(columns=input_feature)\n","\n","categorical_columns = ['dataset_type', 't_model']\n","X_train_dataframe = pd.get_dummies(X_train_dataframe, columns=categorical_columns, prefix=categorical_columns)\n","\n","X_training, X_testing, Y_training, Y_testing = train_test_split(X_train_dataframe, Y_train_dataframe, test_size=0.1, random_state=42)\n","\n","#Define the models\n","model = DecisionTreeRegressor()\n","model.fit(X_training, Y_training)\n","print(\"MSE DecisionTreeRegressor: %.3f\" % mean_squared_error(Y_testing, model.predict(X_testing)))\n","\n","model = DecisionTreeRegressor()\n","model.fit(X_training, Y_training)\n","cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(model, X_training, Y_training, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n","n_scores = absolute(n_scores)\n","# summarize performance\n","print('Kfold Cross Validation DecisionTree MSE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n","\n","model = RandomForestRegressor()\n","model.fit(X_training, Y_training)\n","print(\"MSE RandomForestRegressor: %.3f\" % mean_squared_error(Y_testing, model.predict(X_testing)))\n","\n","model = RandomForestRegressor()\n","model.fit(X_training, Y_training)\n","cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(model, X_training, Y_training, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n","n_scores = absolute(n_scores)\n","# summarize performance\n","print('Kfold Cross Validation RandomForest MSE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n","\n","model = LinearSVR()\n","wrapper = RegressorChain(model)\n","wrapper.fit(X_training, Y_training)\n","print('RegressionChain SVR MSE: %.3f' % mean_squared_error(Y_testing, wrapper.predict(X_testing)))"],"metadata":{"id":"KePxneVKDCgC"},"execution_count":null,"outputs":[]}]}